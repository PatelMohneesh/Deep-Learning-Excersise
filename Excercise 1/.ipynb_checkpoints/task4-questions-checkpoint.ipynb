{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Questions\n",
    "\n",
    "1) What is the difference between the SVM method and a neural network, assuming that both work with the same number of training samples N?\n",
    "\n",
    "   Your answer: \n",
    "   *SVM converges to the global minimum, while in Neural Networks it may converge to the local minimum. \n",
    "   *SVM learns the optimal hyperplane with the maximum margin separating the two linearly separable classes, Whereas, Neural Network's try to learn any of the separating hyperplanes which separate the classes.\n",
    "   Neural Networks are parametric models and SVM are usually non-parametric in nature.\n",
    "   \n",
    "\n",
    "\n",
    "2) Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: 1- Usually in CV we deal with large dimensions of data, and RelU increases the computation over sigmoid and tanh. 2- The gradient of RelU functions are inexpensive to calculate compared to others. 3- RelU accelerates convergence as the gradient is high when the neuron is active\n",
    "   \n",
    "\n",
    "3) Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  the hyperparameters, which stategies you did you use to improve the network, show the results of intermediate and final steps.\n",
    "\n",
    "   Your answer: \n",
    "   \n",
    "   Running the network with initial given parameters, i.e. {hidden_dim=100, reg=0.1, weight_scale=1e-3, num_epoch = 10, batch_size = 500, lr = 1e-2}\n",
    "   \n",
    "5000/49000 loss: 2.1058515071192163\n",
    "10000/49000 loss: 2.146447132192896\n",
    "15000/49000 loss: 2.167035883463707\n",
    "20000/49000 loss: 2.153800109960204\n",
    "25000/49000 loss: 2.1561176032119023\n",
    "30000/49000 loss: 2.12261550467214\n",
    "35000/49000 loss: 2.149883476703213\n",
    "40000/49000 loss: 2.1542496810707576\n",
    "45000/49000 loss: 2.1230945000186754\n",
    "epoch 10: valid acc = 0.198, new learning rate = 0.005987369392383786\n",
    "test acc: 0.1845\n",
    "\n",
    "This meant that the model was underfitting. So I decided to reduce the regularization parameter to .01 and increase the number of hidden dimensions to 200. \n",
    "\n",
    "5000/49000 loss: 2.0422455486715907\n",
    "10000/49000 loss: 2.071403947474287\n",
    "15000/49000 loss: 2.098330162496223\n",
    "20000/49000 loss: 2.0874029892435537\n",
    "25000/49000 loss: 2.0830657342301753\n",
    "30000/49000 loss: 2.0640662986088314\n",
    "35000/49000 loss: 2.083113277051654\n",
    "40000/49000 loss: 2.0773734723932553\n",
    "45000/49000 loss: 2.0589613508915\n",
    "epoch 20: valid acc = 0.207, new learning rate = 0.0035848592240854188\n",
    "test acc: 0.1922\n",
    "   \n",
    "\n",
    "4) **Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation** if you want to use it to find a best set of hyperparameter of the **Linear SVM classification** problem.\n",
    "\n",
    "   Your answer: The k-folds cross-validation technique divides the training data into k non-overlapping subsets. k-1 subsets are used as the training dataset and the remaining subset is used as the test dataset. This process is repeated k times, such that each fold is used as the testing data exactly once. The performance of the model is then evaluated as the mean of the test scores of the k-folds.In order to tune hyperparameters, the above process is repeated with each set of parameter values. The parameter values with the best performance are then chosen as the best hyperparameters. This also eliminates the need for a separate validation set.The regularization parameter of Linear SVM can be trained using this method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
